{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Qwen_generation_1dot5_B_Base.json\") as f:\n",
    "    base_1_5b_output = json.load(f)\n",
    "\n",
    "with open(\"Qwen_generation_7B_Base.json\") as f:\n",
    "    base_7b_output = json.load(f)\n",
    "    \n",
    "with open(\"Qwen_generation_GRPO_NL_7B.json\") as f:\n",
    "    grpo_nl_7b_output = json.load(f)\n",
    "    \n",
    "with open(\"Qwen_generation_SFT_GRPO_1dot5_B.json\") as f:\n",
    "    grpo_sft_1_5b_output = json.load(f)\n",
    "    \n",
    "with open(\"../output/gpt-4o-zero-CoT-combined.json\", 'r') as f:\n",
    "    gpt_4o_zero_cot_output = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 450, 450, 450)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_1_5b_output), len(base_7b_output), len(grpo_nl_7b_output), len(grpo_sft_1_5b_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_3_characters.jsonl\", \"r\") as f:\n",
    "    test_3_characters = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"test_4_characters.jsonl\", \"r\") as f:\n",
    "    test_4_characters = [json.loads(line) for line in f]\n",
    "    \n",
    "with open(\"test_5_characters.jsonl\", \"r\") as f:\n",
    "    test_5_characters = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_3_characters), len(test_4_characters), len(test_5_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answers(text: str) -> dict:\n",
    "    pattern = r\"([A-E]):\\s*(.*?)(?=(?:[A-E]:|$))\"\n",
    "    new_text = text.split(\"<answer>\")[-1].split(\"</answer>\")[0]\n",
    "    text = new_text\n",
    "    \n",
    "    # Use DOTALL to ensure the dot matches newlines as well.\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    answers = {}\n",
    "    for letter, answer in matches:\n",
    "        # First strip whitespace, then remove trailing unwanted characters\n",
    "        cleaned = answer.strip()\n",
    "        # Remove any trailing commas, curly braces, square brackets, newlines, or backslashes\n",
    "        cleaned = re.sub(r'[\\}\\]\\n\\\\,]+$', '', cleaned)\n",
    "        cleaned = cleaned.strip()  # final clean-up\n",
    "        answers[letter] = ('truth' in cleaned)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_range = range(0, 150)\n",
    "test_4_range = range(150, 300)\n",
    "test_5_range = range(300, 450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_output = [extract_answers(base_1_5b_output[key]) for key in base_1_5b_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_metrics(output):\n",
    "    # Load test files\n",
    "    with open(\"test_3_characters.jsonl\", \"r\") as f:\n",
    "        test_3_characters = [json.loads(line) for line in f]\n",
    "\n",
    "    with open(\"test_4_characters.jsonl\", \"r\") as f:\n",
    "        test_4_characters = [json.loads(line) for line in f]\n",
    "        \n",
    "    with open(\"test_5_characters.jsonl\", \"r\") as f:\n",
    "        test_5_characters = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Extract predicted answers using your extraction function\n",
    "    extracted_output = [extract_answers(output[key]) for key in output]\n",
    "    character_3 = extracted_output[:150]\n",
    "    character_4 = extracted_output[150:300]\n",
    "    character_5 = extracted_output[300:]\n",
    "    \n",
    "    # Helper function to compute precision, recall, and F1 for one instance\n",
    "    def compute_metrics(predicted, solution):\n",
    "        tp = sum(predicted[k] and solution[k] for k in predicted)  # True Positives\n",
    "        fp = sum(predicted[k] and not solution[k] for k in predicted)  # False Positives\n",
    "        fn = sum(not predicted[k] and solution[k] for k in predicted)  # False Negatives\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        return precision, recall, f1_score\n",
    "\n",
    "    # Initialize counts and metric sums per case\n",
    "    count_3 = 0\n",
    "    count_4 = 0\n",
    "    count_5 = 0\n",
    "    \n",
    "    precision_sum_3 = 0.0\n",
    "    recall_sum_3 = 0.0\n",
    "    f1_sum_3 = 0.0\n",
    "    \n",
    "    precision_sum_4 = 0.0\n",
    "    recall_sum_4 = 0.0\n",
    "    f1_sum_4 = 0.0\n",
    "    \n",
    "    precision_sum_5 = 0.0\n",
    "    recall_sum_5 = 0.0\n",
    "    f1_sum_5 = 0.0\n",
    "    \n",
    "    # Process 3-character instances\n",
    "    for idx, each in enumerate(character_3):\n",
    "        solution = test_3_characters[idx]['solutions'][0]\n",
    "        if set(each.keys()) == set(solution.keys()) and all(each[k] == solution[k] for k in each.keys()):\n",
    "            count_3 += 1\n",
    "        precision, recall, f1 = compute_metrics(each, solution)\n",
    "        precision_sum_3 += precision\n",
    "        recall_sum_3 += recall\n",
    "        f1_sum_3 += f1\n",
    "\n",
    "    # Process 4-character instances\n",
    "    for idx, each in enumerate(character_4):\n",
    "        solution = test_4_characters[idx]['solutions'][0]\n",
    "        if set(each.keys()) == set(solution.keys()) and all(each[k] == solution[k] for k in each.keys()):\n",
    "            count_4 += 1\n",
    "        precision, recall, f1 = compute_metrics(each, solution)\n",
    "        precision_sum_4 += precision\n",
    "        recall_sum_4 += recall\n",
    "        f1_sum_4 += f1\n",
    "\n",
    "    # Process 5-character instances\n",
    "    for idx, each in enumerate(character_5):\n",
    "        solution = test_5_characters[idx]['solutions'][0]\n",
    "        if set(each.keys()) == set(solution.keys()) and all(each[k] == solution[k] for k in each.keys()):\n",
    "            count_5 += 1\n",
    "        precision, recall, f1 = compute_metrics(each, solution)\n",
    "        precision_sum_5 += precision\n",
    "        recall_sum_5 += recall\n",
    "        f1_sum_5 += f1\n",
    "\n",
    "    # Compute accuracies (percentages)\n",
    "    acc_3 = count_3 / 150 * 100\n",
    "    acc_4 = count_4 / 150 * 100\n",
    "    acc_5 = count_5 / 150 * 100\n",
    "    total_acc = (count_3 + count_4 + count_5) / 450 * 100\n",
    "\n",
    "    # Average metrics per case (converted to percentages)\n",
    "    avg_precision_3 = (precision_sum_3 / 150) * 100\n",
    "    avg_recall_3 = (recall_sum_3 / 150) * 100\n",
    "    avg_f1_3 = (f1_sum_3 / 150) * 100\n",
    "\n",
    "    avg_precision_4 = (precision_sum_4 / 150) * 100\n",
    "    avg_recall_4 = (recall_sum_4 / 150) * 100\n",
    "    avg_f1_4 = (f1_sum_4 / 150) * 100\n",
    "\n",
    "    avg_precision_5 = (precision_sum_5 / 150) * 100\n",
    "    avg_recall_5 = (recall_sum_5 / 150) * 100\n",
    "    avg_f1_5 = (f1_sum_5 / 150) * 100\n",
    "\n",
    "    # Calculate overall averages by summing over all 450 cases\n",
    "    total_precision = (precision_sum_3 + precision_sum_4 + precision_sum_5) / 450 * 100\n",
    "    total_recall = (recall_sum_3 + recall_sum_4 + recall_sum_5) / 450 * 100\n",
    "    total_f1 = (f1_sum_3 + f1_sum_4 + f1_sum_5) / 450 * 100\n",
    "\n",
    "    # Return all metrics rounded to 2 decimal places\n",
    "    return {\n",
    "        \"accuracy\": {\n",
    "            \"character_3\": round(acc_3, 2),\n",
    "            \"character_4\": round(acc_4, 2),\n",
    "            \"character_5\": round(acc_5, 2),\n",
    "            \"overall\": round(total_acc, 2)\n",
    "        },\n",
    "        \"precision\": {\n",
    "            \"character_3\": round(avg_precision_3, 2),\n",
    "            \"character_4\": round(avg_precision_4, 2),\n",
    "            \"character_5\": round(avg_precision_5, 2),\n",
    "            \"overall\": round(total_precision, 2)\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"character_3\": round(avg_recall_3, 2),\n",
    "            \"character_4\": round(avg_recall_4, 2),\n",
    "            \"character_5\": round(avg_recall_5, 2),\n",
    "            \"overall\": round(total_recall, 2)\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"character_3\": round(avg_f1_3, 2),\n",
    "            \"character_4\": round(avg_f1_4, 2),\n",
    "            \"character_5\": round(avg_f1_5, 2),\n",
    "            \"overall\": round(total_f1, 2)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for base_1_5b_output:\n",
      "\n",
      "------------Evaluation Metrics:------------\n",
      "\n",
      "ACCURACY:\n",
      "  Character_3: 29.33%\n",
      "  Character_4: 16.0%\n",
      "  Character_5: 9.33%\n",
      "  Overall: 18.22%\n",
      "\n",
      "PRECISION:\n",
      "  Character_3: 46.78%\n",
      "  Character_4: 40.83%\n",
      "  Character_5: 44.49%\n",
      "  Overall: 44.03%\n",
      "\n",
      "RECALL:\n",
      "  Character_3: 50.89%\n",
      "  Character_4: 45.17%\n",
      "  Character_5: 49.39%\n",
      "  Overall: 48.48%\n",
      "\n",
      "F1:\n",
      "  Character_3: 46.91%\n",
      "  Character_4: 40.63%\n",
      "  Character_5: 44.15%\n",
      "  Overall: 43.9%\n",
      "\n",
      "================================================================================\n",
      "Output for base_7b_output:\n",
      "\n",
      "------------Evaluation Metrics:------------\n",
      "\n",
      "ACCURACY:\n",
      "  Character_3: 24.0%\n",
      "  Character_4: 18.0%\n",
      "  Character_5: 7.33%\n",
      "  Overall: 16.44%\n",
      "\n",
      "PRECISION:\n",
      "  Character_3: 42.44%\n",
      "  Character_4: 42.44%\n",
      "  Character_5: 45.29%\n",
      "  Overall: 43.39%\n",
      "\n",
      "RECALL:\n",
      "  Character_3: 40.78%\n",
      "  Character_4: 45.78%\n",
      "  Character_5: 48.5%\n",
      "  Overall: 45.02%\n",
      "\n",
      "F1:\n",
      "  Character_3: 38.89%\n",
      "  Character_4: 41.13%\n",
      "  Character_5: 43.2%\n",
      "  Overall: 41.07%\n",
      "\n",
      "================================================================================\n",
      "Output for grpo_nl_7b_output:\n",
      "\n",
      "------------Evaluation Metrics:------------\n",
      "\n",
      "ACCURACY:\n",
      "  Character_3: 25.33%\n",
      "  Character_4: 13.33%\n",
      "  Character_5: 10.0%\n",
      "  Overall: 16.22%\n",
      "\n",
      "PRECISION:\n",
      "  Character_3: 41.56%\n",
      "  Character_4: 42.33%\n",
      "  Character_5: 44.61%\n",
      "  Overall: 42.83%\n",
      "\n",
      "RECALL:\n",
      "  Character_3: 39.56%\n",
      "  Character_4: 44.28%\n",
      "  Character_5: 46.44%\n",
      "  Overall: 43.43%\n",
      "\n",
      "F1:\n",
      "  Character_3: 38.51%\n",
      "  Character_4: 39.8%\n",
      "  Character_5: 42.14%\n",
      "  Overall: 40.15%\n",
      "\n",
      "================================================================================\n",
      "Output for grpo_sft_1_5b_output:\n",
      "\n",
      "------------Evaluation Metrics:------------\n",
      "\n",
      "ACCURACY:\n",
      "  Character_3: 55.33%\n",
      "  Character_4: 19.33%\n",
      "  Character_5: 4.0%\n",
      "  Overall: 26.22%\n",
      "\n",
      "PRECISION:\n",
      "  Character_3: 57.78%\n",
      "  Character_4: 46.83%\n",
      "  Character_5: 41.92%\n",
      "  Overall: 48.84%\n",
      "\n",
      "RECALL:\n",
      "  Character_3: 57.67%\n",
      "  Character_4: 48.56%\n",
      "  Character_5: 40.83%\n",
      "  Overall: 49.02%\n",
      "\n",
      "F1:\n",
      "  Character_3: 56.62%\n",
      "  Character_4: 44.97%\n",
      "  Character_5: 39.02%\n",
      "  Overall: 46.87%\n",
      "\n",
      "================================================================================\n",
      "Output for gpt-4o-zero-CoT:\n",
      "\n",
      "------------Evaluation Metrics:------------\n",
      "\n",
      "ACCURACY:\n",
      "  Character_3: 47.33%\n",
      "  Character_4: 42.0%\n",
      "  Character_5: 29.33%\n",
      "  Overall: 39.56%\n",
      "\n",
      "PRECISION:\n",
      "  Character_3: 62.0%\n",
      "  Character_4: 61.11%\n",
      "  Character_5: 52.33%\n",
      "  Overall: 58.48%\n",
      "\n",
      "RECALL:\n",
      "  Character_3: 55.78%\n",
      "  Character_4: 56.5%\n",
      "  Character_5: 50.41%\n",
      "  Overall: 54.23%\n",
      "\n",
      "F1:\n",
      "  Character_3: 57.58%\n",
      "  Character_4: 57.11%\n",
      "  Character_5: 50.03%\n",
      "  Overall: 54.91%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "outputs = {\n",
    "    \"base_1_5b_output\": base_1_5b_output,\n",
    "    \"base_7b_output\": base_7b_output,\n",
    "    \"grpo_nl_7b_output\": grpo_nl_7b_output,\n",
    "    \"grpo_sft_1_5b_output\": grpo_sft_1_5b_output,\n",
    "    \"gpt-4o-zero-CoT\": gpt_4o_zero_cot_output\n",
    "}\n",
    "for output in outputs:\n",
    "    metrics = calculate_metrics(outputs[output])\n",
    "    print(f\"Output for {output}:\\n\")\n",
    "\n",
    "    print(\"------------Evaluation Metrics:------------\\n\")\n",
    "\n",
    "    for metric_type, values in metrics.items():\n",
    "        print(f\"{metric_type.upper()}:\")\n",
    "        for case, value in values.items():\n",
    "            print(f\"  {case.capitalize()}: {value}%\")\n",
    "        print()\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
